description: "mu_activation: tanh"
params:
  seed: ${...seed}

  algo:
    name: custom_agent_player  # custom_agent_player  # a2c_continuous

  model:
    name: custom_model_continuous  # custom_model_continuous  # continuous_a2c_logstd

  network:
    name: actor_critic
    separate: True

    space:
      continuous:
        mu_activation: None
        sigma_activation: None
        mu_init:
          name: default
        sigma_init:
          name: const_initializer
          val: 0 # std = 0.368
        fixed_sigma: True
        action_limitation: [0.15, 0.15, 0.15, 0.15, 0.4, 0.4, 0.4, 0.4, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]

    mlp:
      units: [512, 256, 128] # [512, 256, 128]
      activation: elu
      d2rl: False

      initializer:
        name: default
      regularizer:
        name: None
    # rnn:
    #   name: lstm
    #   units: 128
    #   layers: 1
    #   before_mlp: True
    #   concat_input: True
    #   layer_norm: False


  load_checkpoint: ${if:${...checkpoint},True,False} # flag which sets whether to load the checkpoint
  load_path: ${...checkpoint} # path to the checkpoint to load

  config:
    name: ${resolve_default:A1Record,${....experiment}}
    full_experiment_name: ${.name}
    env_name: rlgpu
    ppo: True
    mixed_precision: True
    normalize_input: True
    normalize_value: True
    normalize_advantage: True
    value_bootstrap: True
    clip_actions: False
    num_actors: ${....task.env.numEnvs}
    reward_shaper:
      scale_value: 1.0
    gamma: 0.99
    tau: 0.95
    e_clip: 0.2
    entropy_coef: 0.01  # 0.001
    learning_rate: 2.e-4 # 2.e-4  # 3.e-4 # overwritten by adaptive lr_schedule
    lr_schedule: adaptive
    kl_threshold: 0.01  # 0.008 # target kl for adaptive lr
    truncate_grads: True
    grad_norm: 1.
    horizon_length: 32  #64 # 1024 # 48 # wsh_annotation: 24
    minibatch_size: 65536  #65536  # 73728  # 36864 # 16384 # 12288
    mini_epochs: 5
    critic_coef: 1.0
    clip_value: True
    seq_len: 4 # only for rnn
    bounds_loss_coef: 0.0  # 0.

#    central_value_config:
#      minibatch_size: 65536
#      mini_epochs: 5
#      learning_rate: 2.e-4
#      lr_schedule: adaptive
#      kl_threshold: 0.01
#      clip_value: True
#      normalize_input: True
#      truncate_grads: True
#
#      network:
#        name: actor_critic
#        central_value: True
#        mlp:
#          units: [512, 256, 128]
#          activation: elu
#          d2rl: False
#          initializer:
#            name: default
#          regularizer:
#            name: None
#        rnn:
#          name: lstm
#          units: 1024
#          layers: 1
#          before_mlp: True
#          layer_norm: True

    max_epochs: ${resolve_default:20000,${....max_iterations}}
    save_best_after: 50  # 100
    score_to_win: 100000
    save_frequency: 50
    print_stats: True
